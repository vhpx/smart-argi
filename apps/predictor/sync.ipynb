{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Initialize common attributes\n",
    "        self.domain = os.getenv(\"DOMAIN\")\n",
    "        self.workspace_id = os.getenv(\"WORKSPACE_ID\")\n",
    "        self.api_key = os.getenv(\"API_KEY\")\n",
    "        \n",
    "        # Dataset IDs\n",
    "        self.world_bank_dataset_id = os.getenv(\"WORLD_BANK_DATASET_ID\")\n",
    "        self.agro_gov_dataset_id = os.getenv(\"AGRO_GOV_DATASET_ID\")\n",
    "        self.vbma_dataset_id = os.getenv(\"VBMA_DATASET_ID\")\n",
    "        \n",
    "        # Common headers\n",
    "        self.headers = {\n",
    "            \"API_KEY\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    def _make_request(self, dataset_id):\n",
    "        \"\"\"Make HTTP request to API endpoint\"\"\"\n",
    "        conn = http.client.HTTPSConnection(self.domain)\n",
    "        conn.request(\n",
    "            \"GET\",\n",
    "            f\"/api/v1/workspaces/{self.workspace_id}/datasets/{dataset_id}/full\",\n",
    "            headers=self.headers\n",
    "        )\n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        return json.loads(data.decode(\"utf-8\"))\n",
    "\n",
    "    def _flatten_data(self, rows):\n",
    "        \"\"\"Flatten nested JSON data structure\"\"\"\n",
    "        flattened_data = []\n",
    "        for row in rows:\n",
    "            cells = row[\"cells\"]\n",
    "            cells[\"row_id\"] = row[\"row_id\"]\n",
    "            flattened_data.append(cells)\n",
    "        return flattened_data\n",
    "\n",
    "    def load_price_data(self):\n",
    "        \"\"\"Load and process World Bank rice price data\"\"\"\n",
    "        parsed_data = self._make_request(self.world_bank_dataset_id)\n",
    "        flattened_data = self._flatten_data(parsed_data[\"data\"])\n",
    "        \n",
    "        # Convert to DataFrame and clean\n",
    "        price_df = pd.DataFrame(flattened_data)\n",
    "        price_df = price_df.replace(\"…\", pd.NA)\n",
    "        price_df = price_df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        \n",
    "        # Keep only Rice and Date columns\n",
    "        price_df = price_df[[\"Rice, Viet Namese 5%\", \"Date\"]]\n",
    "        \n",
    "        # Convert Date format from YYYYMM to YYYY-MM-DD\n",
    "        price_df['Date'] = pd.to_datetime(price_df['Date'].astype(str).str.replace('M', '-'), format='%Y-%m') + pd.offsets.MonthBegin(0)\n",
    "        \n",
    "        # Sort by Date\n",
    "        return price_df.sort_values('Date')\n",
    "\n",
    "    def load_news_data(self):\n",
    "        \"\"\"Load and process agricultural news data\"\"\"\n",
    "        parsed_data = self._make_request(self.agro_gov_dataset_id)\n",
    "        flattened_data = self._flatten_data(parsed_data[\"data\"])\n",
    "        \n",
    "        # Convert to DataFrame and clean\n",
    "        news_df = pd.DataFrame(flattened_data)\n",
    "        news_df = news_df.replace(\"…\", pd.NA)\n",
    "        news_df = news_df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        \n",
    "        # Convert DATE column to datetime\n",
    "        news_df['DATE'] = pd.to_datetime(news_df['DATE'], format='%d | %m | %Y')\n",
    "        \n",
    "        # Sort by DATE\n",
    "        return news_df.sort_values('DATE')\n",
    "\n",
    "    def load_vbma_data(self):\n",
    "        \"\"\"Load and process VBMA data\"\"\"\n",
    "        parsed_data = self._make_request(self.vbma_dataset_id)\n",
    "        flattened_data = self._flatten_data(parsed_data[\"data\"])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        # Clean data - first replace \"...\" with NA, then replace \"N/A\" and \"-\" with 0\n",
    "        df = df.replace(\"…\", pd.NA)\n",
    "        df = df.replace([\"N/A\", \"-\"], 0)\n",
    "        df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        \n",
    "        # Drop row_id before transposing\n",
    "        df = df.drop('row_id', axis=1)\n",
    "        \n",
    "        # Transpose and set headers\n",
    "        df = df.transpose()\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Reset index and rename columns\n",
    "        df = df.reset_index().rename(columns={'index': 'ds'})\n",
    "        \n",
    "        # Clean date strings\n",
    "        df['ds'] = df['ds'].str.replace('- ', '')\n",
    "        df['ds'] = df['ds'].str.replace('T', '')\n",
    "        df['ds'] = df['ds'].str.replace(r'\\([^)]*\\)', '', regex=True).str.strip()\n",
    "        \n",
    "        # Convert to datetime\n",
    "        df['ds'] = pd.to_datetime(df['ds'], format='mixed') + pd.offsets.MonthBegin(0)\n",
    "        # Drop the first row:\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = DataLoader()\n",
    "    price_df = loader.load_price_data()\n",
    "    news_df = loader.load_news_data()\n",
    "    vbma_df = loader.load_vbma_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICAL MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoTheta, AutoCES, AutoTBATS\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stats_df from price_df and add unique_id column\n",
    "stats_df = price_df.copy()\n",
    "stats_df = stats_df.rename(columns={'Rice, Viet Namese 5%': 'y', 'Date': 'ds'})\n",
    "stats_df['unique_id'] = 'stats'\n",
    "stats_df = stats_df[['ds', 'y', 'unique_id']]  # Reorder columns\n",
    "stats_df = stats_df.reset_index(drop=True)  # Reset index to increase incrementally\n",
    "\n",
    "# Handle missing values by forward filling and then backward filling\n",
    "stats_df['y'] = stats_df['y'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Number of periods to forecast ahead\n",
    "forecast_horizon = 6\n",
    "# Size of each rolling window step\n",
    "step_size = 1\n",
    "# Total number of rolling windows for cross-validation\n",
    "n_windows = 36\n",
    "\n",
    "# Initialize Ray for parallel processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Define the models and forecaster\n",
    "season_length = 12  # annual seasonality for monthly data\n",
    "# List of statistical forecasting models with seasonal components\n",
    "models = [\n",
    "    AutoARIMA(season_length=season_length),  # Automated ARIMA model selection\n",
    "    AutoETS(season_length=season_length),    # Automated Exponential Smoothing\n",
    "    AutoTheta(season_length=season_length),  # Automated Theta method\n",
    "    AutoCES(season_length=season_length)     # Automated Complex Exponential Smoothing\n",
    "]\n",
    "\n",
    "# Create StatsForecast object with parallel processing\n",
    "def get_stats_forecaster():\n",
    "    \"\"\"\n",
    "    Creates and returns a StatsForecast object with the defined models.\n",
    "    \n",
    "    Returns:\n",
    "        StatsForecast: Configured forecaster with parallel processing enabled\n",
    "    \"\"\"\n",
    "    return StatsForecast(models=models, freq='M', n_jobs=-1)\n",
    "\n",
    "def prepare_data(df, use_scaler=False):\n",
    "    \"\"\"\n",
    "    Prepares data for forecasting by handling data types and optional scaling.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with time series data\n",
    "        use_scaler (bool): Whether to apply MinMax scaling to the target variable\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Prepared DataFrame, Fitted scaler or None if scaling not used)\n",
    "    \"\"\"\n",
    "    # Ensure 'y' column is numeric\n",
    "    df = df.copy()\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    \n",
    "    # Handle any remaining missing values\n",
    "    df['y'] = df['y'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Apply MinMax scaling if requested\n",
    "    scaler = None\n",
    "    if use_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        df['y'] = scaler.fit_transform(df[['y']])\n",
    "    \n",
    "    # Split the data into train and test sets based on rolling window parameters\n",
    "    train_size = len(df) - n_windows * step_size\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "    \n",
    "    return pd.concat([train_df, test_df]), scaler\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates multiple performance metrics for forecast evaluation.\n",
    "    \n",
    "    Args:\n",
    "        actual (array-like): True values\n",
    "        predicted (array-like): Predicted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RMSE, Directional Accuracy, Turning Point Accuracy)\n",
    "    \"\"\"\n",
    "    actual = np.asarray(actual).flatten()\n",
    "    predicted = np.asarray(predicted).flatten()\n",
    "\n",
    "    # Root Mean Square Error\n",
    "    rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # Directional Accuracy - measures correct prediction of up/down movements\n",
    "    actual_diff = np.diff(actual)\n",
    "    pred_diff = np.diff(predicted)\n",
    "    directional_accuracy = np.mean((actual_diff * pred_diff) > 0)\n",
    "    \n",
    "    # Turning Point Accuracy - measures correct prediction of trend changes\n",
    "    actual_turns = (actual_diff[:-1] * actual_diff[1:]) < 0\n",
    "    pred_turns = (pred_diff[:-1] * pred_diff[1:]) < 0\n",
    "    turning_point_accuracy = np.mean(actual_turns == pred_turns)\n",
    "    \n",
    "    return rmse, directional_accuracy, turning_point_accuracy\n",
    "\n",
    "@ray.remote\n",
    "def run_experiment(df, model_names, use_scaler=False):\n",
    "    \"\"\"\n",
    "    Runs forecasting experiment with cross-validation for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with time series data\n",
    "        model_names (list): List of model names to evaluate\n",
    "        use_scaler (bool): Whether to apply MinMax scaling\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Results DataFrame with metrics, Cross-validation DataFrame with predictions)\n",
    "    \"\"\"\n",
    "    # Initialize forecaster and prepare data\n",
    "    stats_forecaster = get_stats_forecaster()\n",
    "    prepared_df, scaler = prepare_data(df, use_scaler)\n",
    "    \n",
    "    # Prepare for cross-validation\n",
    "    cv_df = prepared_df[['ds', 'y', 'unique_id']].copy()\n",
    "    cv_df['y'] = cv_df['y'].astype(float)\n",
    "\n",
    "    # Perform rolling window cross-validation\n",
    "    crossvalidation_df = stats_forecaster.cross_validation(\n",
    "        df=cv_df,\n",
    "        h=forecast_horizon,\n",
    "        step_size=step_size,\n",
    "        n_windows=n_windows\n",
    "    )\n",
    "\n",
    "    # Inverse transform predictions if scaling was applied\n",
    "    if scaler:\n",
    "        crossvalidation_df['y'] = scaler.inverse_transform(crossvalidation_df[['y']])\n",
    "        for model in model_names:\n",
    "            if model in crossvalidation_df.columns:\n",
    "                crossvalidation_df[model] = scaler.inverse_transform(crossvalidation_df[[model]])\n",
    "\n",
    "    # Calculate performance metrics for each model\n",
    "    results = []\n",
    "    for model in model_names:\n",
    "        if model in crossvalidation_df.columns:\n",
    "            rmse, dir_acc, turn_acc = calculate_metrics(\n",
    "                crossvalidation_df['y'].values,\n",
    "                crossvalidation_df[model].values\n",
    "            )\n",
    "            \n",
    "            # Calculate weighted score (equal weights for all metrics)\n",
    "            weighted_score = (rmse + (1 - dir_acc) + (1 - turn_acc)) / 3\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model,\n",
    "                'RMSE': rmse,\n",
    "                'Directional_Accuracy': dir_acc,\n",
    "                'Turning_Point_Accuracy': turn_acc,\n",
    "                'Weighted_Score': weighted_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), crossvalidation_df\n",
    "\n",
    "# Define statistical models to evaluate\n",
    "model_names = ['AutoARIMA', 'AutoETS', 'AutoTheta', 'CES']\n",
    "\n",
    "# Run parallel experiments with and without data scaling\n",
    "experiment_ref_no_scale = run_experiment.remote(stats_df, model_names, use_scaler=False)\n",
    "experiment_ref_with_scale = run_experiment.remote(stats_df, model_names, use_scaler=True)\n",
    "\n",
    "# Collect results from parallel processes\n",
    "results_df_no_scale, crossvalidation_df_no_scale = ray.get(experiment_ref_no_scale)\n",
    "results_df_with_scale, crossvalidation_df_with_scale = ray.get(experiment_ref_with_scale)\n",
    "\n",
    "# Display performance metrics for both scaling approaches\n",
    "print(\"\\nModel Performance Metrics (No Scaling):\")\n",
    "print(results_df_no_scale.to_string(index=False))\n",
    "print(\"\\nModel Performance Metrics (With MinMax Scaling):\")\n",
    "print(results_df_with_scale.to_string(index=False))\n",
    "\n",
    "# Identify best performing models based on weighted score\n",
    "best_model_no_scale = results_df_no_scale.loc[results_df_no_scale['Weighted_Score'].idxmin(), 'Model']\n",
    "best_model_with_scale = results_df_with_scale.loc[results_df_with_scale['Weighted_Score'].idxmin(), 'Model']\n",
    "print(f\"\\nBest Model (No Scaling): {best_model_no_scale}\")\n",
    "print(f\"Best Model (With MinMax Scaling): {best_model_with_scale}\")\n",
    "\n",
    "# Clean up Ray resources\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "import multiprocessing\n",
    "from math import sqrt\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# MLForecast\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.auto import (\n",
    "    AutoMLForecast,\n",
    "    AutoElasticNet, \n",
    "    AutoXGBoost,\n",
    "    AutoLightGBM,\n",
    "    AutoCatboost\n",
    ")\n",
    "from mlforecast.target_transforms import LocalStandardScaler\n",
    "from mlforecast.lag_transforms import ExponentiallyWeightedMean, RollingMean\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from utilsforecast.plotting import plot_series\n",
    "\n",
    "# Core forecasting utilities\n",
    "from coreforecast.scalers import LocalStandardScaler, LocalMinMaxScaler\n",
    "from coreforecast.grouped_array import GroupedArray\n",
    "\n",
    "# Set up multiprocessing and seeds\n",
    "print(multiprocessing.cpu_count())\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "set_seeds()\n",
    "def catboost_model_params(trial: optuna.Trial):\n",
    "    return {\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate multiple performance metrics for forecasting evaluation.\n",
    "    \n",
    "    Args:\n",
    "        actual (array-like): The actual/true values\n",
    "        predicted (array-like): The predicted/forecasted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - rmse (float): Root Mean Square Error\n",
    "            - directional_accuracy (float): Proportion of correctly predicted directions (0-1)\n",
    "            - turning_point_accuracy (float): Proportion of correctly predicted turning points (0-1)\n",
    "            - weighted_score (float): Combined score weighing all three metrics equally\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays and flatten\n",
    "    actual = np.asarray(actual).flatten()\n",
    "    predicted = np.asarray(predicted).flatten()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # Calculate directional accuracy (proportion of correctly predicted up/down movements)\n",
    "    actual_diff = np.diff(actual)\n",
    "    pred_diff = np.diff(predicted)\n",
    "    directional_accuracy = np.mean((actual_diff * pred_diff) > 0)\n",
    "    \n",
    "    # Calculate turning point accuracy (proportion of correctly predicted trend changes)\n",
    "    actual_turns = (actual_diff[:-1] * actual_diff[1:]) < 0  # True when direction changes\n",
    "    pred_turns = (pred_diff[:-1] * pred_diff[1:]) < 0\n",
    "    turning_point_accuracy = np.mean(actual_turns == pred_turns)\n",
    "    \n",
    "    # Calculate weighted score - lower is better\n",
    "    # Combines RMSE with penalties for poor directional and turning point accuracy\n",
    "    weighted_score = (rmse + (1 - directional_accuracy) + (1 - turning_point_accuracy)) / 3\n",
    "    \n",
    "    return rmse, directional_accuracy, turning_point_accuracy, weighted_score\n",
    "\n",
    "\n",
    "def run_forecasting_pipeline(stats_df, horizon=6, step_size=1, n_windows=36):\n",
    "    \"\"\"Run an automated machine learning forecasting pipeline with multiple models.\n",
    "    \n",
    "    This function implements a complete forecasting workflow including:\n",
    "    - Train/test splitting\n",
    "    - Data preprocessing and scaling\n",
    "    - Model training with cross-validation\n",
    "    - Prediction generation\n",
    "    - Performance evaluation and visualization\n",
    "    \n",
    "    Args:\n",
    "        stats_df (pd.DataFrame): Input dataframe containing target variable 'y',\n",
    "            datetime column 'ds', ID column 'unique_id' and optional macro features\n",
    "        horizon (int, optional): Number of future periods to forecast. Defaults to 3.\n",
    "        step_size (int, optional): Number of periods between cross-validation windows. Defaults to 3.\n",
    "        n_windows (int, optional): Number of cross-validation windows. Defaults to 16.\n",
    "            \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - auto_mlf (AutoMLForecast): The fitted forecasting model\n",
    "            - predictions (pd.DataFrame): Future predictions\n",
    "            - cv_results (dict): Cross-validation results for each model\n",
    "            - metrics_df (pd.DataFrame): Performance metrics comparison\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    # Test set size is determined by number of windows * step size\n",
    "    train_size = len(stats_df) - n_windows * step_size\n",
    "    train_df = stats_df[:train_size].copy()\n",
    "    test_df = stats_df[train_size:].copy()  \n",
    "\n",
    "    # Basic preprocessing - fill missing values with 0\n",
    "    processed_df = stats_df.copy()\n",
    "    processed_df.fillna(0)\n",
    "\n",
    "    # Identify any exogenous (macro) features by excluding standard columns\n",
    "    macro_features = processed_df.columns.difference(['unique_id', 'ds', 'y'])\n",
    "    has_exog = len(macro_features) > 0\n",
    "\n",
    "    # Scale macro features if present using local min-max scaling\n",
    "    if has_exog:\n",
    "        scaler = LocalMinMaxScaler()\n",
    "        \n",
    "        # First scale training data\n",
    "        for feature in macro_features:\n",
    "            train_values = train_df[feature].values\n",
    "            indptr = np.array([0, len(train_values)])\n",
    "            grouped_train = GroupedArray(train_values, indptr)\n",
    "            scaled_train_values = scaler.fit_transform(grouped_train)\n",
    "            train_df[feature] = scaled_train_values\n",
    "\n",
    "        # Then scale full dataset using fitted scaler\n",
    "        for feature in macro_features:\n",
    "            full_values = processed_df[feature].values\n",
    "            indptr = np.array([0, len(full_values)])\n",
    "            grouped_full = GroupedArray(full_values, indptr)\n",
    "            scaled_full_values = scaler.transform(grouped_full)\n",
    "            processed_df[feature] = scaled_full_values\n",
    "\n",
    "    # Initialize dictionary of models to evaluate\n",
    "    models = {\n",
    "        'elasticnet': AutoElasticNet(),  # Linear model with L1/L2 regularization\n",
    "        'xgboost': AutoXGBoost(),        # Gradient boosting with trees\n",
    "        'lightgbm': AutoLightGBM(),      # Light gradient boosting\n",
    "        'catboost': AutoCatboost(config = catboost_model_params)  # Categorical boosting\n",
    "    }\n",
    "\n",
    "    # Configure automated ML forecasting framework\n",
    "    auto_mlf = AutoMLForecast(\n",
    "        models=models,\n",
    "        freq='MS',  # Monthly frequency\n",
    "        season_length=12,  # Annual seasonality\n",
    "        fit_config=lambda trial: {\n",
    "            'static_features': [],\n",
    "            'dropna': True,\n",
    "            'keep_last_n': None\n",
    "        },\n",
    "        num_threads=12  # Parallel processing\n",
    "    )\n",
    "\n",
    "    # Fit models with cross-validation\n",
    "    print(\"Performing optimization and cross-validation...\")\n",
    "    auto_mlf.fit(\n",
    "        train_df,\n",
    "        n_windows=2,\n",
    "        h=1,\n",
    "        num_samples=20,\n",
    "        step_size=1\n",
    "    )\n",
    "\n",
    "    # Generate future prediction dataframe\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    any_model = next(iter(auto_mlf.models_.values()))\n",
    "    future_df = any_model.make_future_dataframe(h=horizon)\n",
    "    \n",
    "    # Handle future macro features if present\n",
    "    if has_exog:\n",
    "        # Get last known values for each series\n",
    "        last_dates = stats_df.groupby('unique_id')['ds'].max()\n",
    "        future_values = []\n",
    "        \n",
    "        # Create future macro data using last known values\n",
    "        for idx, row in future_df.iterrows():\n",
    "            uid = row['unique_id']\n",
    "            last_known_values = stats_df[stats_df['unique_id'] == uid].loc[\n",
    "                stats_df['ds'] == last_dates[uid], \n",
    "                macro_features\n",
    "            ].iloc[0]\n",
    "            \n",
    "            future_values.append({\n",
    "                'unique_id': uid,\n",
    "                'ds': row['ds'],\n",
    "                **last_known_values\n",
    "            })\n",
    "        \n",
    "        # Scale future macro features\n",
    "        future_macro_df = pd.DataFrame(future_values)\n",
    "        for feature in macro_features:\n",
    "            future_values = future_macro_df[feature].values\n",
    "            indptr = np.array([0, len(future_values)])\n",
    "            grouped_future = GroupedArray(future_values, indptr)\n",
    "            scaled_future_values = scaler.transform(grouped_future)\n",
    "            future_macro_df[feature] = scaled_future_values\n",
    "        \n",
    "        # Generate predictions with exogenous features\n",
    "        predictions = auto_mlf.predict(horizon, X_df=future_macro_df)\n",
    "    else:\n",
    "        # Generate predictions without exogenous features\n",
    "        predictions = auto_mlf.predict(horizon)\n",
    "\n",
    "    # Evaluate models using cross-validation\n",
    "    cv_results = {}\n",
    "    metrics = {}\n",
    "\n",
    "    # Loop through each model for evaluation\n",
    "    for model_name, model in auto_mlf.models_.items():\n",
    "        # Perform cross-validation on last 48 periods\n",
    "        cv_df = model.cross_validation(\n",
    "            df=processed_df,\n",
    "            n_windows=36,\n",
    "            h=6,\n",
    "            step_size=1,\n",
    "            static_features=[],\n",
    "            dropna=True,\n",
    "        )\n",
    "        cv_results[model_name] = cv_df\n",
    "        actual = cv_df['y']\n",
    "        predicted = cv_df[model_name]\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        rmse, dir_acc, turn_acc, weighted_score = calculate_metrics(actual, predicted)\n",
    "        metrics[model_name] = {\n",
    "            'RMSE': rmse,\n",
    "            'Directional Accuracy': dir_acc,\n",
    "            'Turning Point Accuracy': turn_acc,\n",
    "            'Weighted Score': weighted_score\n",
    "        }\n",
    "\n",
    "        # Create evaluation plots\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        print(f\"\\nMetrics for {model_name}:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Directional Accuracy: {dir_acc:.4f}\")\n",
    "        print(f\"Turning Point Accuracy: {turn_acc:.4f}\")\n",
    "        print(f\"Weighted Score: {weighted_score:.4f}\")\n",
    "        \n",
    "        # Print value ranges for validation\n",
    "        print(f\"\\nValue ranges for {model_name}:\")\n",
    "        print(\"Original data range:\", stats_df['y'].min(), \"-\", stats_df['y'].max())\n",
    "        print(\"Predicted data range:\", cv_df[model_name].min(), \"-\", cv_df[model_name].max())\n",
    "        print(\"Time range:\", cv_df['ds'].min(), \"-\", cv_df['ds'].max())\n",
    "    # Create comparison metrics dataframe\n",
    "    metrics_df = pd.DataFrame(metrics).round(4)\n",
    "    print(\"\\nModel Comparison Metrics:\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    # Identify best performing model based on weighted score\n",
    "    best_model = min(metrics.items(), key=lambda x: x[1]['Weighted Score'])\n",
    "    print(f\"\\nBest Model: {best_model[0]} (Weighted Score: {best_model[1]['Weighted Score']:.4f})\")\n",
    "\n",
    "    return auto_mlf, predictions, cv_results, metrics_df\n",
    "\n",
    "# Run the forecasting pipeline\n",
    "auto_mlf, predictions, cv_results, metrics_df = run_forecasting_pipeline(stats_df, horizon=6, step_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
